<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>Report</title></head>
<body><p><strong>Udacity&#39;s Deep Reinforcement Learning Nanodegree</strong></p>
<h1>Report on Navigation Project</h1>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h2>Learning Algorithm</h2>
<p>The agent is learning the policy using reinforcement learning method by follow the below steps;</p>
<ol>
<li><p>Choose an action based on the current state using the policy</p>
</li>
<li><p>Apply the chosen action to the environment to receive;</p>
<ul>
<li>Reward, which tells whether the action is good or bad,</li>
<li>Next state, which is the resulting state after taking the action,</li>
<li>Done, which tells whether the episode is finished or not</li>

</ul>
</li>
<li><p>Put the information acquired at Step 2 into the replay buffer, and execute learning algorithm every given steps (in this case, 4 steps)</p>
<ul>
<li><p>For learning, it randomly picks samples for mini-batch from the replay buffer,</p>
</li>
<li><p>Calculate the target return using the target network as;</p>
<ul>
<li>target return = reward + \gamma max \hat{Q}(next state, action, \theta_{target}) ​ </li>

</ul>
</li>
<li><p>Calculate the expected return using the local network as;</p>
<ul>
<li>expected return = max Q(state, \theta_{local})​ </li>

</ul>
</li>
<li><p>Then, compute loss using MSE between the target and expected returns, and run stochastic gradient descent to update the weights of the local network</p>
<ul>
<li>Loss = MSE(target return, expected return)</li>

</ul>
</li>
<li><p> In this implementation, the weights of the target network is being updated gradually rather than copying all the weights of the local network into the target network.</p>
<ul>
<li>\theta_{target} = \tau \times \theta_{local} + (1-\tau) \times \theta_{target}  </li>

</ul>
</li>
<li><p>Please refer to <a href='https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf'>DQN paper</a> for further detail.</p>
</li>

</ul>
</li>

</ol>
<p>Code snippet of this process is given below and the full code is in <a href='./Navigation.ipynb'>Navigation.ipynb</a> and  <a href='./dqn_agent.py'>dqn_agent.py</a></p>
<pre><code class='language-py' lang='py'># Step 1 ~ Step 2
action = agent.act(state, eps)
env_info = env.step(action)[brain_name]
next_state, reward, done = extract_info(env_info)
agent.step(state, action, reward, next_state, done)
state = next_state
score += reward
if done:
	break
	
# Step 3
states, actions, rewards, next_states, dones = experiences

Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1) 
Q_targets = rewards + (gamma * Q_targets_next * (1-dones))
Q_expected = self.qnetwork_local(states).gather(1, actions) 

loss = F.mse_loss(Q_expected, Q_targets)
self.optimizer.zero_grad()
loss.backward()
self.optimizer.step()

# ------------------- update target network ------------------- #
self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)    
</code></pre>
<p>&nbsp;</p>
<h2>Network Architecture for Deep Q-Network (DQN)</h2>
<p>Neural network for DQN is a multilayer perceptron with 3 hidden layers which has 64 hidden units each. ReLU activation function is applied to each hidden layer. Dropout is defined and applied to the output of activation outputs, but the dropout probability is set as 0 which disables dropout for now.</p>
<pre><code class='language-py' lang='py'>class QNetwork(nn.Module):
    &quot;&quot;&quot;Actor (Policy) Model.&quot;&quot;&quot;

    def __init__(self, state_size, action_size, seed, fc1_units=32, fc2_units=32, fc3_units=32):
        &quot;&quot;&quot;Initialize parameters and build model.
        Params
        ======
            state_size (int): Dimension of each state
            action_size (int): Dimension of each action
            seed (int): Random seed
        &quot;&quot;&quot;
        super(QNetwork, self).__init__()
        self.seed = torch.manual_seed(seed)
        self.fc1 = nn.Linear(state_size, fc1_units)
        self.fc2 = nn.Linear(fc1_units, fc2_units)
        self.fc3 = nn.Linear(fc2_units, fc3_units)
        self.fc4 = nn.Linear(fc3_units, action_size)
        self.dropout = nn.Dropout(0)

    def forward(self, state):
        x = self.dropout(F.relu(self.fc1(state)))
        x = self.dropout(F.relu(self.fc2(x)))
        x = self.dropout(F.relu(self.fc3(x)))
        return self.fc4(x)
</code></pre>
<p>&nbsp;</p>
<h2>Experiments</h2>
<p>&nbsp;</p>
<pre><code>Episode 100	Average Score: 0.45 in 0.79 sec
Episode 200	Average Score: 2.90 in 0.81 sec
Episode 300	Average Score: 6.97 in 0.82 sec
Episode 400	Average Score: 10.02 in 0.82 sec
Episode 500	Average Score: 14.02 in 0.85 sec
Episode 600	Average Score: 14.36 in 0.83 sec
Episode 694	Average Score: 15.05 in 0.82 sec
Environment solved in 594 episodes!	Average Score: 15.05	Min Score: 7.00
</code></pre>
<p>&nbsp;</p>
<h2>Plot of Rewards</h2>
<p>The best performing agent has been trained with 694 episodes which gives the average rewards over previous 100 episodes as 15.05. The below plot shows the cumulative rewards and the average scores.</p>
<p><img src='./rewards.png' alt='Rewards' referrerPolicy='no-referrer' /></p>
<p>As seen in the above plot, the average score starts to be saturated after about 500 episodes. Why? I need to investigate further :)</p>
<h2>Ideas for Future Work</h2>
<p>Different methods of DQN could be applied to find which one effectively improves the performance. They are</p>
<ul>
<li><a href='https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf'>Double DQN</a></li>
<li>[Prioritized Experience Replay]</li>
<li>[Dueling DQN]</li>
<li>[Rainbow]</li>

</ul>
<p>&nbsp;</p>
</body>
</html>